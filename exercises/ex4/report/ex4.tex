%Exercise 4 LaTeX report for TMA 4280
\documentclass[fontsize=11pt,paper=a4,titlepage]{report}
% \usepackage{float} %dunno yet??, probably replaced by amsfonts
\usepackage{listings}
\usepackage{color}
\usepackage{amsfonts}	%For \mathbb
\usepackage{caption}	%Dunno yet
\usepackage{todonotes}	%For \todo

\lstset{ %
language=C,						% choose the language of the code
basicstyle=\footnotesize,		% the size of the fonts that are used for the code
numbers=left,					% where to put the line-numbers
numberstyle=\footnotesize,		% the size of the fonts that are used for the line-numbers
stepnumber=1,					% the step between two line-numbers. If it is 1 each line will be numbered
numbersep=5pt,					% how far the line-numbers are from the code
backgroundcolor=\color{white},	% choose the background color. You must add \usepackage{color}
showspaces=false,				% show spaces adding particular underscores
showstringspaces=false,			% underline spaces within strings
showtabs=false,					% show tabs within strings adding particular underscores
frame=single,					% adds a frame around the code
tabsize=4,						% sets default tabsize to 2 spaces
captionpos=b,					% sets the caption-position to bottom
breaklines=true,				% sets automatic line breaking
breakatwhitespace=false,		% sets if automatic breaks should only happen at whitespace
escapeinside={\%*}{*)}			% if you want to add a comment within your code
}

\begin{document}
%\input{config} %config.tex file in same directory
\begin{center}


{\huge Problem set 4}\\[0.5cm]

\textsc{\LARGE TMA4280 \newline Introduction to supercomputing - \newline Mandatory problem set}\\[2cm]


\begin{table}[h]
\centering
\begin{tabular}{ccc}
	\textsc{Christian CHAVEZ}	&	\textsc{Mireia DUASO}	&	\textsc{Erlend SIGHOLT}
\end{tabular}
\end{table}

\large{\today}
\vfill
\section*{Abstract}
\todo[inline]{EMPTY!}

\end{center}

\addtocounter{chapter}{1}

\clearpage
\section{Introduction}
For the programming belonging to this problem set, see $\textit{ex4.c}$ file in
the zipped archive attached to this report. This program calculates the
difference between the sum of a finite Hyperharmonic series and the convergence
sum for an infinite Hyperharmonic series. This program is able to compute the
difference between the two in the following scenarioes;

\begin{enumerate}
	\item[(i)]{Using one thread and a single process on a single processing core
for the program's computations.}
	\item[(ii)]{Using one process and multiple threads concurrently on a single
processing core for the program's computations.}
	\item[(iii)]{Using multiple processing cores concurrently for the program's
computations.}
	\item[(iv)]{Using multiple threads on multiple processing cores concurrently for
the program's computations.}
\end{enumerate}

\todo{FIX CAPTION!}

\section{Series $S_n$}

A \textit{Hyperharmonic} series is a mathematical series with the following form;

\begin{figure}[h]
	\begin{equation}
		\sum_{i=1}^{\infty} \frac{1}{i^p}
	\end{equation}
	\caption{Hyperharmonic series}
	\label{HyperSeries}
\end{figure}

In this problemset, the relevant form of the abovementioned Hyperharmonic figure
that we're interested in is displayed below;

\begin{figure}[h]
	\begin{equation}
		S_n = \sum_{i=1}^{n} \frac{1}{i^2}
	\end{equation}
	\caption{The $n$-th partial Hyperharmonic sum when $p = 2$}
	\label{HyperSum}
\end{figure}

This problem set wants us to write a program with $n = 2^k$, where $k$ is given
as parameter at \todo{Confirm phrasing of program initiation}program start. This
problem set also specifies $p$ to be a constant; $p = 2$. With $p = 2$, the sum
of this Hyperharmonic series converges to $\frac{\pi^2}{6}$.

The objective of the problem set is to compute the difference between $S =
\lim_{n \to \infty}S_n$ and the partial sum $S_n$ as figured in (\ref{HyperSum}).
The problem set wants the program to compute these differences with $n = 2^k$,
with $\{k \in \mathbb{N} : k \in [3, 14]\}$.

\section{Concurrency Implementation}

The program was developed in iterations. The first iteration ran on a single
thread in a single process on a single processor core, in a non-concurrent
sequential fashion. The second iteration introduced running the program as
mentioned in item (ii) in \cite{RunMode}. This was enabled by putting the following
line into our \textit{ex4.c} file.

% \begin{lslisting}
% 	#pragma omp parallel for schedule(dynamic, 5) reduction(+:sum)
% \end{lslisting}

The third iteration focused on combining items (iii) and (iv) from \cite{RunMode}.
Since OpenMP is enabled through the use of pragmas, making the code run with MPI
was then only a matter of choosing the right compiler and compilation switches.
Hence, in effect the third iteration only focused on implementing MPI into the
program.

To utilize MPI, the program does need to use a subset of the available MPI
function calls; $\textit{MPI\_Scatter()}$, $\textit{MPI\_Send()}$,
$\textit{MPI\_Reduce()}$, $\textit{MPI\_Gather()}$, $\textit{MPI\_Init()}$,
$\textit{MPI\_Finalize()}$, and so on. Not all of these are necessary, but a
minimum subset of these are. Such as, $\textit{MPI\_Init()}$ and $\textit{
MPI\_Finalize()}$, and then one of the $\textit{MPI\_Scatter()}$ or
$\textit{MPI\_Send()}$ alternatives to send data among the processes used by
MPI. Likewise there are multiple alternatives for $\textit{MPI\_Gather()}$ and
$\textit{MPI\_Reduce()}$, but you only need one of the alternatives to receive
data from the other processes used by MPI.

In our solution, implemented in the attached code, we utilized the following MPI
functions:

\begin{itemize}
	\item{$\textit{MPI\_Init()}$ and $\textit{
MPI\_Finalize()}$ through the provided \textit{common.c} \textit{common.h}
framework.}
	\item{$\textit{MPI\_Scatter()}$ and $\textit{MPI\_Reduce()}$ were used to
send and receive data between the processes respectively.
$\textit{MPI\_Reduce()}$ also aided us by performing the summing necessary
between the processes for us, storing the result in a specified variable.}
\end{itemize}

These function calls are considered more convenient for the purpose of this
problem set. This due to the fact that the problem is relatively simple, and
easy to load-balance. Hence, no complicated send/receive inter-communication
between the processes is needed to coordinate the concurrency. All the summing
is done independently of any other summing (besides the final summing performed
by $\textit{MPI\_Reduce()}$).

Had we implemented MPI through the use of a $\textit{MPI\_Send()}$ alternative
and/or a $\textit{MPI\_Gather()}$ alternative, we would have to modify the code
such that it could take care of the concurrency through the use of something

\todo[inline]{ERLEND!!! This is where you wanted to come in. The two above
paragraphs was what we had written when you told us you wanted to take over this
part. }

\todo[inline]{ANSWER QUESTION 6!!!!!}

\todo[inline]{ANSWER QUESTION 7!!!!!}

\section{Program Load}

The computational load (mathematical complexity) of our program can be given as
the amount of \textit{FLoating-point OPerations} (FLOPs) it needs to complete
for a given input. Since this program computes the sum of a Hyperharmonic finite
series, below we detail the math behind the program's computations.

\subsection{Mathematical computation}

The mathematical operation needed to generate each term of the Hyperharmonic
series is as follows;

\begin{equation}
	v_i = \frac{1}{i^2}
\end{equation}

Analyzing how many FLOPs are required for generating a term $v_i$, we see that
it requires a multiplication ($i\cdot i = i^2 = X$) and a division
($\frac{1}{X}$). A total of 2 FLOPs. So to generate a Hyperharmonic series of
$n$ terms, requires $2n$ FLOPs. Below we detail the formula used in the program
for this problem set when calculating the sum of our finite length Hyperharmonic
series.

The total amount of FLOPs needed for computing (\ref{HyperSum}) will then be $2n + n = 3n$.
Thus, as we have use $k$ to define $n=2^k$ the computational cost of our program
can be described as; $2^k\cdot 3$ FLOPs.

\subsection{Load-balancing}

The program is very well load-balanced for this problem set. The problem set
specifies the length of our vectors (representing the Hyperharmonic series) to
be $2^k$ long, and that our MPI implementation should only run on a system using
$P$ processors where $P$ is a power of 2.

Hence, the summation cost of the double-values in the vector of length $2^k$ can
always be divided equally among $P = 2^m$ processors, as long as $k\geq m$. The
parallellism is possible due to the fact that all partial sums are completely
independent of each other. \newline

\todo[inline]{Maybe mention that MPI requires node 0 (rank == 0) to do some extra pre-processing?}

Therefore, we believe that you can safely conclude that within the parameters
given in this problem set, this problem is very attractive to solve through
parallell processing.



% \input{chapters/implementation}
% \input{chapters/methodology}
% \input{chapters/results}
% \input{chapters/conclusion}

%\lstlistoflistings
% \listoffigures
% \listoftables

\end{document}