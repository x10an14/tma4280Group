%Exercise 4 LaTeX report for TMA 4280
\documentclass[fontsize=11pt,paper=a4,titlepage]{report}
\input{config} %config.tex file in same directory

\begin{document}

\begin{center}

%\lstlistoflistings
% \listoffigures
% \listoftables

{\huge Problem set 4}\\[0.5cm]

\textsc{\LARGE TMA4280}\\[0.5cm]
\textsc{\large Introduction to supercomputing -}\\
\textsc{\large Mandatory problem set}\\[0.6cm]

\begin{table}[h]
\centering
\begin{tabular}{ccc}
	\textsc{Christian CHAVEZ}	&	\textsc{Mireia DUASO}	&	\textsc{Erlend SIGHOLT}
\end{tabular}
\end{table}

\large{\today}
\vfill
\section*{Abstract}
This report describes a program made to compute the sum of a Hyperharmonic series, through a finite approximation, and comparing said sum to the known convergence of the infinite series.

It covers the serial implementation of an algorithm, and exploitation of parallelization through the OpenMP API and OpenMPI Library, in order to improve the efficiency and speed of the program.

Finally it covers the results from utilizing parallelization, and analyzes the utility of using parallelization to speed up this task.
\todo[inline]{EMPTY!}

\end{center}

\addtocounter{chapter}{1}

\clearpage
\section{Introduction}

For the programming belonging to this problem set, see $\textit{ex4.c}$ file in
the zipped archive attached to this report. This program calculates the
difference between the sum of a finite Hyperharmonic series and the convergence
sum for an infinite Hyperharmonic series. This program is able to compute the
difference between the two in the following scenarioes;

\todo[inline]{Check if it's correct to say ``potentially multiple cores'', or if
I misunderstood how MPI is implemented on kongull.}

\begin{table}[h]
	\begin{tabularx}{\linewidth}{c|X|X|}
			OpenMP/MPI	& OpenMP off & OpenMP on	\\ \hline
			MPI off		& Using one thread and a single process on a single
processing core for the program's computations. & Using one process with
multiple threads on a single processing core for the program's
computations. \\ \hline
			MPI on		& Using multiple processes potentially running on
multiple cores concurrently for the program's computations. & Usiqsub unauthorized request msg=group aclng multiple
threads in multiple processes running on potentially multiple cores concurrently
for the program's computations. \\ \hline
	\end{tabularx}
	\caption{Different runmodes for the program in this problem set.}
	\label{tab:RunModes}
\end{table}

\section{Series $S_n$}

Below, in equation~\ref{eq:HyperSeries}, the form of a~\textit{Hyperharmonic}
series is shown.

\begin{equation}
	\sum_{i=1}^{\infty} \frac{1}{i^p}
	\label{eq:HyperSeries}
\end{equation}

However, this problem set only asks for the sum of a certain subset of
Hyperharmonic series, when $p = 2$. Below, equation~\ref{eq:HyperSum} shows how
to calculate the sum of such a Hyperharmonic series.

\begin{equation}
	S_n = \sum_{i=1}^{n} \frac{1}{i^2}
	\label{eq:HyperSum}
\end{equation}

This problem set asks for a program that can calculate the sum of Hyperharmonic
series of the form shown in equation~\ref{eq:HyperSeries}, of finite length $n$.

Since the problem set wants us to develop a program which can concurrently
calculate this sum in parallel on $P = 2^m$ discrete processors, this problem
set also specifies that $n$ should follow certain conditions; $n = 2^k$, where
$k$ is given as parameter on the form $\{k \in \mathbb{N} : k \in [3, 14]\}$.

\begin{equation}
	\frac{\pi^2}{6} = \lim_{n \to \infty}S_n = \sum_{i=1}^{\infty} \frac{1}{i^2}
	\label{eq:HyperActualSum}
\end{equation}

So, after having detailed that $S = \lim_{n \to \infty}S_n$ converges to $\frac{
\pi^2}{6}$, and exactly which finite lengths of $n$ the problem set is
interested in. It is our understanding that this problem set is interested in
the students developing the abovementioned parallel program to calculate the
difference between the sum given in equation~\ref{eq:HyperActualSum} and the sum
when $n$ is of a given length, decided by $k$.

\section{Concurrency Implementation}

The program was developed in iterations. The first iteration ran on a single
thread in a single process on a single processor core, in a non-concurrent
sequential fashion. The second iteration introduced running the program as
mentioned in the top-right cell of table~\ref{tab:RunModes}. This was enabled by
putting the following line into our \textit{ex4.c} file.

\lstinputlisting[firstline=17,lastline=21,firstnumber=17,label=lst:OpenMP]{
../ex4.c}

The third iteration focused on enabling the program to run in the runmode
corresponding to the bottom-right cell of table~\ref{tab:RunModes}.
Since OpenMP is enabled through the use of pragmas, making the code run with MPI
was then only a matter of choosing the right compiler and compilation switches.
Hence, in effect the third iteration only focused on implementing MPI into the
program.

To utilize MPI, the program does need to use a subset of the available MPI
function calls; $\textit{MPI\_Scatter()}$, $\textit{MPI\_Send()}$,
$\textit{MPI\_Reduce()}$, $\textit{MPI\_Gather()}$, $\textit{MPI\_Init()}$,
$\textit{MPI\_Finalize()}$, and so on. Not all of these are necessary, but a
minimum subset of these are. Such as, $\textit{MPI\_Init()}$ and $\textit{
MPI\_Finalize()}$, and then one of the $\textit{MPI\_Scatter()}$ or
$\textit{MPI\_Send()}$ alternatives to send data among the processes used by
MPI. Likewise there are multiple alternatives for $\textit{MPI\_Gather()}$ and
$\textit{MPI\_Reduce()}$, but you only need one of the alternatives to receive
data from the other processes used by MPI.

In our solution, implemented in the attached code, we utilized the following MPI
functions:
\todo[inline]{Possibly change the wording in the description of Scatter/Reduce. They both include sending and receiving. Change to distribute/gather instead?}
\begin{itemize}
	\item{$\textit{MPI\_Init()}$ and $\textit{
MPI\_Finalize()}$ through the provided \textit{common.c} \textit{common.h}
framework.}
	\item{$\textit{MPI\_Scatter()}$ and $\textit{MPI\_Reduce()}$ were used to
send and receive data between the processes respectively.
$\textit{MPI\_Reduce()}$ also aided us by performing the summing necessary
between the processes for us, storing the result in a specified variable.}
\end{itemize}

These function calls are considered more convenient considering the purpose of
this problem set. This due to the fact that the problem is relatively simple,
and easy to load-balance. Hence, no complicated send/receive inter-communication
between the processes is needed to coordinate the concurrency. All the summing
is done independently of any other summing (besides the final summing performed
by $\textit{MPI\_Reduce()}$).

MPI could also have been used through the use of the $\textit{MPI\_Send()}$
and/or $\textit{MPI\_Gather()}$ functions. This would have provided more
detailed controll over the data (location and communication), but this is not
neccessary for solving this problem and might actually be harmful to efficiency.
This is due to potential added overhead for communication and variable
assignment. (It is here assumed that the implementation of
$\textit{MPI\_Reduce()}$ is more efficient than any manual summing and use of
Send/Gather would be).

\todo[inline]{ANSWER QUESTION 6!!!!! (Check final section first!)}

\section{Program Load}

\subsection{Memory requirement}
\label{subsec:MemReq}

The program developed for this problem set requires a certain amount of
available memory to run. There are several factors and variables to take into
account, which can all differ according to the runmodes of the program (level of
parallelism or lack thereof) and circumstance.

Since the problem set specifies the circumstance $n \gg 1$, we will ignore the
neglegible extra memory needed for OpenMP. This is also because a set of threads
managed by OpenMP, specifically performing a summation (as shown in
subsection~\ref{lst:OpenMP}), will have a neglegible impact on the program's
memory usage.

Hence, below there are two equations where we detail how we believe the \textit{
non-neglegible} memory requirement should behave when run in a single process,
or in $P$ processes with input size decided by variable $k$ as a deciding factor
in either case.

\todo[inline]{Bother to count supVars? Erlend, you who's got OpenMP experience,
what do you think? If so, update/fix footnote 1 and above paragraphs.}

\begin{equation}
	noMPImem = supVars\footnote{supVars = support Variables. This amount is
static, and very neglegible compared to the sum describing the size of the
vector.} + \sum_{i=1}^{2^k}
sizeof(double)
	\label{eq:noMPI-Mem}
\end{equation}

\begin{equation}
	MPImem = noMPImem + (P-1)\footnote{P = $2^m$ = amount of processes the
program is run on concurrently though MPI.} \cdot supVars + P \cdot
\sum_{i=1}^{2^{k-m}} sizeof(double)
	\label{eq:MPI-Mem}
\end{equation}

As detailed above in equation~\ref{eq:MPI-Mem} (parallell circumstance), you can
see that the memory requirement ``noMPImem'' detailed in
equation~\ref{eq:noMPI-Mem} (serial circumstance), will always be present.
However, the sum term in equation~\ref{eq:MPI-Mem} can be thought of as a
duplication of the sum representing the memory required for the length of the
vector representing the Hyperharmonic series in equation~\ref{eq:noMPI-Mem}.

The only difference being that its memory requirement is spread across the $P$
processes.

\todo[inline]{Need to know if MPI spreads across processors or only processes
before I can finish this!!! (Question 7)}

\subsection{Mathematical computation}

The computational load (mathematical complexity) of our program can be given as
the amount of \textit{\textbf{Fl}oating-point \textbf{Op}erations} (FLOPs) it
needs to complete for a given input. Since this program computes the sum of a
Hyperharmonic finite series, below we detail the math behind the program's
computations.

The mathematical operation needed to generate each term of the Hyperharmonic
series is as follows;

\begin{equation}
	v_i = \frac{1}{i^2}
\end{equation}

Analyzing how many FLOPs are required for generating a term $v_i$, we see that
it requires a multiplication ($i\cdot i = i^2 = X$) and a division
($\frac{1}{X}$). A total of 2 FLOPs. So to generate a Hyperharmonic series of
$n$ terms, requires $2n$ FLOPs. Below we detail the formula used in the program
for this problem set when calculating the sum of our finite length Hyperharmonic
series.

The total amount of FLOPs needed for computing (\ref{eq:HyperSum}) will then be
$2n + n = 3n$. Thus, as we have use $k$ to define $n=2^k$ the computational cost
of our program can be described as; $2^k\cdot 3$ FLOPs.

\subsection{Load-balancing}

The program developed is very well load-balanced for this problem set. The
problem set specifies the length of the vectors (representing the Hyperharmonic
series) to be $2^k$ long, and that our MPI implementation should only run on a
system using $P$ processors where $P$ is a power of 2.

It should be mentioned that the program does some pre-processing serially,
mandated by the task and by MPI. The initialization is neccessary and
unavoidable, and the generation of the vector would probably not benefit much
(if at all) by parallelization.

Either way, the neglegible computations involved in the initialization of the
problem will be negilible compared to the overall  runtime for problem sizes of
any considerable size\footnote{$n \gg 1$, as  referenced in
subsection~\ref{subsec:MemReq}.}. This stands equally true for the  initial
generation of the vector representing the Hyperharmonic series of the problem
set. In any attempt at parallelizing the vector generation, the  necessary
overhead would dominate any gain.

Hence, the summation cost of the double-values in the vector of length $2^k$ can
always be divided equally among $P = 2^m$ processors, as long as $k\geq m$. The
parallellism is possible due to the fact that all partial sums are completely
independent of each other. \newline

Therefore, we believe that you can safely conclude that within the parameters
given in this problem set, this problem is very attractive to solve through
parallell processing.

\section{Results}

\todo[inline]{Implement this? I imagine this is where question 6 should be...}

\end{document}